{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44dcf862",
   "metadata": {},
   "source": [
    "## Authors: Pablo Mollá, Pavlo Poliuha and Junjie Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3710942-d267-47db-b6cf-15cce55fef50",
   "metadata": {},
   "source": [
    "# Neural network: first experiments with a linear model\n",
    "\n",
    "In this lab exercise we will code a neural network using numpy, without a neural network library.\n",
    "Next week, the lab exercise will be to extend this program with hidden layers and activation functions.\n",
    "\n",
    "The task is digit recognition: the neural network has to predict which digit in $\\{0...9\\}$ is written in the input picture. We will use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a standard benchmark in machine learning.\n",
    "\n",
    "The model is a simple linear  classifier $o = \\operatorname{softmax}(Wx + b)$ where:\n",
    "* $x$ is an input image that is represented as a column vector, each value being the \"color\" of a pixel\n",
    "* $W$ and $b$ are the parameters of the classifier\n",
    "* $\\operatorname{softmax}$ transforms the output weight (logits) into probabilities\n",
    "* $o$ is column vector that contains the probability of each category\n",
    "\n",
    "We will train this model via stochastic gradient descent by minimizing the negative log-likelihood of the data:\n",
    "$$\n",
    "    \\hat{W}, \\hat{b} = \\operatorname{argmin}_{W, b} \\sum_{x, y} - \\log p(y | x)\n",
    "$$\n",
    "Although this is a linear model, it classifies raw data without any manual feature extraction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780e4c48-dd03-40d8-857c-759b997f9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs that we will use\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# To load the data we will use the script of Gaetan Marceau Caron\n",
    "# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n",
    "import dataset_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e162f3f-18cb-4fd3-b003-da463965f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mnist dataset \n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    # this link doesn't work any more,\n",
    "    # seach on google for the file \"mnist.pkl.gz\"\n",
    "    # and download it\n",
    "    !wget https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fca3e04-965a-44b1-9419-0b532c3352b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371bc90-6323-4109-8419-93f6fa309cae",
   "metadata": {},
   "source": [
    "Each dataset is a list with two elemets:\n",
    "* data[0] contains images\n",
    "* data[1] contains labels\n",
    "\n",
    "Data is stored as numpy.ndarray. You can use data[0][i] to retrieve image number i and data[1][i] to retrieve its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22acb142-3f16-4bb4-9704-1b243f27df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25d0f156fb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 900\n",
    "label = train_data[1][index]\n",
    "picture = train_data[0][index]\n",
    "\n",
    "print(\"Label: %i\" % label)\n",
    "plt.imshow(picture.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65aa50a-ea30-4708-b9ca-7dae904687f0",
   "metadata": {},
   "source": [
    "**Question 1:** What are the characteristics of training data? (number of samples, dimension of input, number of labels)\n",
    "\n",
    "The documentation of ndarray class is available here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b17b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91efaaab-2287-417d-9a62-b4a631b0b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDimDataset(data):\n",
    "    X, Y = data\n",
    "    \n",
    "    # Number of samples\n",
    "    n_training = X.shape[0]  \n",
    "    \n",
    "    # Dimension of input/features per sample\n",
    "    n_feature = X.shape[1]\n",
    "    \n",
    "    # Number of unique labels/classes\n",
    "    n_label = len(np.unique(Y))\n",
    "    \n",
    "    return n_training, n_feature, n_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8938454-4727-41c7-802c-3955fba9f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 50000\n",
      "Dimension of input/features: 784\n",
      "Number of labels/classes: 10\n"
     ]
    }
   ],
   "source": [
    "n_training, n_feature, n_label = getDimDataset(train_data)\n",
    "\n",
    "print(f\"Number of training samples: {n_training}\")\n",
    "print(f\"Dimension of input/features: {n_feature}\")\n",
    "print(f\"Number of labels/classes: {n_label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbad24",
   "metadata": {},
   "source": [
    "- The number 50,000 (50k) represents the total number of training samples in the dataset. In the context of the MNIST dataset, each of these samples is an image of a handwritten digit.\n",
    "\n",
    "- The number 784 represents the dimension of the input features for each sample in the dataset. Specifically, for the MNIST dataset, which consists of grayscale images of handwritten digits (0 through 9), each image is 28 pixels by 28 pixels. When these images are unrolled into a single vector (which is a common preprocessing step for feeding images into a machine learning model), they become a vector with 28×28=784 elements.\n",
    "\n",
    "- The number 10 represents the total number of unique labels or classes in the dataset. Since MNIST is a dataset of handwritten digits from 0 to 9, there are 10 possible classes that each image can be classified into, corresponding to each digit. This means the dataset is a multi-class classification problem with 10 distinct classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da32ee-fa5c-48c8-af0f-580eba03f6c5",
   "metadata": {},
   "source": [
    "# 1. Building functions\n",
    "\n",
    "We now need to build functions that are required for the neural network.\n",
    "$$\n",
    "    o = \\operatorname{softmax}(Wx + b) \\\\\n",
    "    L(x, y) = -\\log p(y | x) = -\\log o[y]\n",
    "$$\n",
    "\n",
    "Note that in numpy, operator @ is used for matrix multiplication while * is used for element-wise multiplication.\n",
    "The documentation for linear algebra in numpy is available here: https://docs.scipy.org/doc/numpy/reference/routines.linalg.html\n",
    "\n",
    "The first operation is the affine transformation $v = Wx + b$.\n",
    "To compute the gradient, it is often convenient to write the forward pass as $v[i] = b[i] + \\sum_j W[i, j] x[j]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086d756",
   "metadata": {},
   "source": [
    "Let's explain the following code:\n",
    "\n",
    "1. `W` is a 2D NumPy array with a shape of (5, 10). It will represent the weights of the neural network layer where we have 10 neurons (output dimension) and each neuron is connected to an input of dimension 5.\n",
    "\n",
    "2. `x` is a 2D NumPy array with a shape of (1, 5). This represents a single input sample with 5 features.\n",
    "\n",
    "3. `np.matmul(W.transpose(1,0), x.transpose(1,0))` performs matrix multiplication between the transposed `W` and the transposed `x`. The resulting matrix multiplication is valid because the inner dimensions (5 from `W`'s transpose and 5 from `x`'s transpose) match. The operation will result in a new matrix with a shape of (10, 1), which is the result of applying the transposed weight matrix to the input vector.\n",
    "\n",
    "This matrix multiplication can be thought of as taking the input `x` and transforming it into a new space as defined by the transposed weight matrix `W`. If `W` were the weights of a neural network layer, then this operation would be computing the activation of the previous layer (because of the transpose) given the current layer's input `x`. However, this kind of operation (using the transpose of the weights matrix) isn't typical in a forward pass of a neural network—it's more akin to what happens during the backpropagation step when gradients are being computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4989ef8-ec12-487a-988b-a3d657d454a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.74614385],\n",
       "       [-1.64078612],\n",
       "       [ 0.45543486],\n",
       "       [ 0.54637237],\n",
       "       [ 0.67491943],\n",
       "       [-1.01043933],\n",
       "       [ 0.23227993],\n",
       "       [ 0.51032964],\n",
       "       [ 1.03048434],\n",
       "       [-1.460326  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.randn(5,10)\n",
    "x = np.random.randn(1,5)\n",
    "print(W.transpose(1,0).shape)\n",
    "np.matmul(W.transpose(1,0), x.transpose(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c167d320-9a24-43de-9311-0846e2f1a280",
   "metadata": {},
   "source": [
    "**Question 2:**  Complete the two functions `affine_transform` and `backward_affine_transform`. The last function computes the gradient of the loss function according to weights of the linear module. The gradient of the loss according to output of the linear module is given as last parameter of the function backward_affine_transform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c73659b",
   "metadata": {},
   "source": [
    "### Analytical Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb677fe",
   "metadata": {},
   "source": [
    "To provide a detailed analytical explanation of the backward pass through an affine transformation using the given operations, let's consider the following:\n",
    "\n",
    "1. **Forward Pass:**\n",
    "   - Affine Transformation: $y = Wx + b$\n",
    "   \n",
    "   - Where $x$ is our input vector (size $d \\times 1$), $W$ is the weight matrix (size $m \\times d$), $b$ is the bias vector (size $m \\times 1$), and $y$ is the output vector (size $m \\times 1$).\n",
    "\n",
    "2. **Backward Pass:**\n",
    "   - Let's consider the loss function $L$. We are interested in how changes in $W$ and $b$ affect $L$, i.e., we want to find $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$.\n",
    "   \n",
    "   - Given the incoming gradient of the loss with respect to the output of the affine layer, $\\frac{\\partial L}{\\partial y}$ (denoted by $g$ and of size $m \\times 1$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2d154",
   "metadata": {},
   "source": [
    "### Gradient with respect to W ($\\frac{\\partial L}{\\partial W}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370a619",
   "metadata": {},
   "source": [
    "Given that $L$ is a function of $y$ (i.e., $L(y)$), and $y$ is a function of $W$, applying the chain rule yields:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W} $$\n",
    "\n",
    "Where:\n",
    "- $\\frac{\\partial L}{\\partial y}$ is $g$, our incoming gradient.\n",
    "\n",
    "- $\\frac{\\partial y}{\\partial W}$ can be thought of as how each component of $y$ changes with a small change in $W$.\n",
    "\n",
    "Since $y = Wx + b$, a small change in $W$ leads to a change in $y$ exactly proportional to $x$.\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial W} = x^T $$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\underbrace{\\frac{\\partial L}{\\partial y}}_{g} \\cdot \\underbrace{\\frac{\\partial y}{\\partial W}}_{x^T} = g \\cdot x^T $$\n",
    "\n",
    "\n",
    "This is why we calculate $g_W = np.dot(g, x.T)$ in the code to get the gradient of $L\\$ with respect to $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff35976",
   "metadata": {},
   "source": [
    "### Gradient with respect to b ($\\frac{\\partial L}{\\partial b}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d5304",
   "metadata": {},
   "source": [
    "- Similarly, since $b$ directly adds to $y$, the change in $L$ with respect to a change in $b$ is simply the gradient $g$ itself, aggregated across all samples. \n",
    "\n",
    "- Since $y = Wx + b$, we obtain:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial b} = 1 \\text{ (vector of 1s)} $$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\underbrace{\\frac{\\partial L}{\\partial y}}_{g} \\cdot \\underbrace{\\frac{\\partial y}{\\partial W}}_{1} = g \\cdot 1 = \\sum g $$\n",
    "\n",
    "\n",
    "Thus, $g_b = np.sum(g, axis=0, keepdims=True)$ calculates the sum of gradients across the batch (if in a batch setting) or simply keeps $g$ in the correct shape for updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee5da951-0405-4aaa-9207-b02aff3116cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation Transformation\n",
    "def affine_transform(W, b, x):\n",
    "    # Input:\n",
    "    # - W: projection matrix\n",
    "    # - b: bias\n",
    "    # - x: input features\n",
    "    # Output:\n",
    "    # - vector\n",
    "    return W @ x.T + b\n",
    "\n",
    "\n",
    "# Backward Propagation Tranformation\n",
    "def backward_affine_transform(W, b, x, g):\n",
    "    # Input:\n",
    "    # - W: projection matrix\n",
    "    # - b: bias\n",
    "    # - x: input features\n",
    "    # - g: incoming gradient\n",
    "    # Output:\n",
    "    # - g_W: gradient wrt W\n",
    "    # - g_b: gradient wrt b\n",
    "    g_W = np.outer(g, x.T)\n",
    "    g_b = g*(np.ones(b.shape))    \n",
    "    return g_W, g_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80dfc98c-5ec4-42ac-9d5d-dc671eec3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.asarray([[ 0.63024213,  0.53679375, -0.92079597],\n",
    " [-0.1155045,   0.62780356, -0.67961305],\n",
    " [ 0.08465286, -0.06561815, -0.39778322],\n",
    " [ 0.8242268,   0.58907262, -0.52208052],\n",
    " [-0.43894227, -0.56993247,  0.09520727]])\n",
    "\n",
    "\n",
    "b = np.asarray([ 0.42706842,  0.69636598, -0.85611933, -0.08682553,  0.83160079])\n",
    "x = np.asarray([-0.32809223, -0.54751413,  0.81949319])\n",
    "\n",
    "o_gold = np.asarray([-0.82819732, -0.16640748, -1.17394705, -1.10761496,  1.36568213])\n",
    "g = np.asarray([-0.08938868,  0.44083873, -0.2260743,  -0.96196726, -0.53428805])\n",
    "g_W_gold = np.asarray([[ 0.02932773,  0.04894156, -0.07325341],\n",
    " [-0.14463576, -0.24136543,  0.36126434],\n",
    " [ 0.07417322,  0.12377887, -0.18526635],\n",
    " [ 0.31561399,  0.52669067, -0.78832562],\n",
    " [ 0.17529576,  0.29253025, -0.43784542]])\n",
    "g_b_gold = np.asarray([-0.08938868,  0.44083873, -0.2260743,  -0.96196726, -0.53428805])\n",
    "\n",
    "\n",
    "# quick test of the forward pass\n",
    "o = affine_transform(W, b, x)\n",
    "if o.shape != o_gold.shape:\n",
    "    raise RuntimeError(\"Unexpected output dimension: got %s, expected %s\" % (str(o.shape), str(o_gold.shape)))\n",
    "if not np.allclose(o, o_gold):\n",
    "    raise RuntimeError(\"Output of the affine_transform function is incorrect\")\n",
    "    \n",
    "# quick test if the backward pass\n",
    "g_W, g_b = backward_affine_transform(W, b, x, g)\n",
    "if g_W.shape != g_W_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension for W: got %s, expected %s\" % (str(g_W.shape), str(g_W_gold.shape)))\n",
    "if g_b.shape != g_b_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension for b: got %s, expected %s\" % (str(g_b.shape), str(g_b_gold.shape)))\n",
    "if not np.allclose(g_W, g_W_gold):\n",
    "    raise RuntimeError(\"Gradient of W is incorrect\")\n",
    "if not np.allclose(g_b, g_b_gold):\n",
    "    raise RuntimeError(\"Gradient of b is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed21a0ba-a70e-4d81-a2ef-ffff48ca7b29",
   "metadata": {},
   "source": [
    "The softmax function:\n",
    "$$\n",
    "     o = \\operatorname{softmax}(w)\n",
    "$$\n",
    "where $w$ is a vector of logits in $\\mathbb R$ and $o$ a vector of probabilities such that:\n",
    "$$\n",
    "    o[i] = \\frac{\\exp(w[i])}{\\sum_j \\exp(w[j])}\n",
    "$$\n",
    "We do not need to implement the backward for this experiment.\n",
    "\n",
    "**Question 3** Implement the function softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3079bfae-61fd-4bb3-aa5b-5e8f560334cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # Input:\n",
    "    # - x: vector of logits\n",
    "    # Output\n",
    "    # - vector of probabilities\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72da73-9779-4374-9fdc-16e35e0181e9",
   "metadata": {},
   "source": [
    "**WARNING:** is your implementation numerically stable?\n",
    "\n",
    "The $\\exp$ function results in computations that overflows (i.e. results in numbers that cannot be represented with floating point numbers).\n",
    "Therefore, it is always convenient to use the following trick to improve stability: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b75087-b8bd-4d9c-bc6a-65f4d68114ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Example for testing the numerical stability of softmax\n",
    "# It should return [1., 0. ,0.], not [nan, 0., 0.]\n",
    "z = [1000000,1,100]\n",
    "print(softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55067f3f-c6cf-408a-a574-ae805bb804e8",
   "metadata": {},
   "source": [
    "**Question 4**: From the result of the cell above, what can you say about the softmax output, even when it is stable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bc2e5",
   "metadata": {},
   "source": [
    "In the previous example, we have used the softmax function on the input z = [1000000,1,100]. \n",
    "\n",
    "- Without the numerical stability trick, exponentiating such large numbers would result in overflow, potentially leading to NaN (not a number) values. However, with the implemented trick, the softmax function can still output a valid probability distribution [1., 0., 0.]. \n",
    "\n",
    "- This result tells us that even for inputs with large values, the output probabilities are calculated correctly without numerical overflow or underflow.\n",
    "\n",
    "So, even when the softmax function is numerically stable:\n",
    "\n",
    "- The outputs are valid probabilities that sum up to 1.\n",
    "- The output does not suffer from overflow or underflow issues.\n",
    "- The function can handle a wide range of input values, including very large or small logits, without returning erroneous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b0495c-9cc4-4877-a14b-1b3a170661a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just too simple test for the softmax function\n",
    "x = np.asarray([0.92424884, -0.92381088, -0.74666024, -0.87705478, -0.54797015])\n",
    "y_gold = np.asarray([0.57467369, 0.09053556, 0.10808233, 0.09486917, 0.13183925])\n",
    "\n",
    "y = softmax(x)\n",
    "if not np.allclose(y, y_gold):\n",
    "    raise RuntimeError(\"Output of the softmax function is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aabf403-e4a2-4acb-a835-0ae5b7d7a2b4",
   "metadata": {},
   "source": [
    "Finally, we build the loss function and its gradient for training the network.\n",
    "\n",
    "The loss function is the negative log-likelihood defined as:\n",
    "$$\n",
    "    \\mathcal L(x, gold) = -\\log \\frac{\\exp(x[gold])}{\\sum_j \\exp(x[j])} = -x[gold] + \\log \\sum_j \\exp(x[j])\n",
    "$$\n",
    "This function is also called the cross-entropy loss (in Pytorch, different names are used dependending if the inputs are probabilities or raw logits).\n",
    "\n",
    "Similarly to the softmax, we have to rely on the log-sum-exp trick to stabilize the computation: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "\n",
    "**Question 5:** Implement the forward and backward function for the negative loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242cd0e7",
   "metadata": {},
   "source": [
    "### Example of Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16aca0",
   "metadata": {},
   "source": [
    "#### Forward Pass:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31242a",
   "metadata": {},
   "source": [
    "Input:\n",
    "\n",
    "- Logits from the neural network: x = [2.0, 1.0, 0.5]\n",
    "- True class (gold): gold = 0 (assuming classes are numbered 0, 1, and 2)\n",
    "\n",
    "##### 1. First, we compute the softmax probabilities to convert the logits into probabilities:\n",
    "\n",
    "- Calculate exp(x[j]) for each class j.\n",
    "- Sum these exponentials to get sum_j exp(x[j]).\n",
    "\n",
    "The probability for the true class gold is exp(x[gold]) / sum_j exp(x[j]).\n",
    "\n",
    "##### 2. Next, we compute the Negative Log-Likelihood:\n",
    "\n",
    "- Take the negative log of the probability of the true class: -log(exp(x[gold]) / sum_j exp(x[j])).\n",
    "- Simplify using the properties of logarithms to get -x[gold] + log(sum_j exp(x[j])).\n",
    "\n",
    "##### Example Computation:\n",
    "\n",
    "1. Softmax Probabilities\n",
    "- softmax_probabilities = exp([2.0, 1.0, 0.5]) / sum(exp([2.0, 1.0, 0.5]))\n",
    "- softmax_probabilities ≈ [0.6, 0.3, 0.1] (these are not exact values, just approximations for illustration)\n",
    "\n",
    "2. Negative Log-Likelihood\n",
    "- NLL = -log(0.6) ≈ -(-0.51) ≈ 0.51 (using a common logarithm approximation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd77823",
   "metadata": {},
   "source": [
    "#### Backward Pass:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42878ccb",
   "metadata": {},
   "source": [
    "- Now, we compute the gradient of the loss function with respect to the input logits. This gradient indicates how much a small change in each logit would change the loss.\n",
    "\n",
    "1. Subtract 1 from the probability of the true class (since we want to decrease the loss for the correct class, making its probability closer to 1).\n",
    "2. Leave the probabilities of the incorrect classes as they are (since increasing their probabilities would increase the loss).\n",
    "\n",
    "##### Example Computation:\n",
    "\n",
    "- grad = softmax_probabilities (copy probabilities)\n",
    "- grad[gold] = grad[gold] - 1 (adjust for true class)\n",
    "- grad ≈ [0.6 - 1, 0.3, 0.1]\n",
    "- grad ≈ [-0.4, 0.3, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14cd59b9-7a7f-446a-81fa-8eaf3abf6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - x: vector of logits\n",
    "# - gold: index of the gold class\n",
    "# Output:\n",
    "# - scalare equal to -log(softmax(x)[gold])\n",
    "def nll(x, gold):\n",
    "    return -np.log(softmax(x)[gold])\n",
    "\n",
    "# Input:\n",
    "# - x: vector of logits\n",
    "# - gold: index of the gold class\n",
    "# - gradient (scalar)\n",
    "# Output:\n",
    "# - gradient wrt x\n",
    "def backward_nll(x, gold, g):\n",
    "    g_x = softmax(x)\n",
    "    g_x[gold] -= 1\n",
    "    g_x *= g\n",
    "    return g_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5df5bdcc-6452-434e-b343-87c08abf0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "x = np.asarray([-0.13590009, -0.83649656,  0.03130881,  0.42559402,  0.08488182])\n",
    "y_gold = 1.5695014420179738\n",
    "g_gold = np.asarray([ 0.17609875,  0.08739591, -0.79185107,  0.30875221,  0.2196042 ])\n",
    "\n",
    "y = nll(x, 2)\n",
    "g = backward_nll(x, 2, 1.)\n",
    "\n",
    "if not np.allclose(y, y_gold):\n",
    "    raise RuntimeError(\"Output is incorrect\")\n",
    "\n",
    "if g.shape != g_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension: got %s, expected %s\" % (str(g.shape), str(g_gold.shape)))\n",
    "if not np.allclose(g, g_gold):\n",
    "    raise RuntimeError(\"Gradient is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0d56d-82ea-478c-a72e-042e5bc63240",
   "metadata": {},
   "source": [
    "The following code tests the implementation of the gradient using finite-difference approximation, see:\n",
    "- https://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/\n",
    "\n",
    "Your implementation should pass this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f66cd42-9cae-496d-84df-5deaf41817c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is python re-implementation of the test from the Dynet library\n",
    "# https://github.com/clab/dynet/blob/master/dynet/grad-check.cc\n",
    "\n",
    "def is_almost_equal(grad, computed_grad):\n",
    "    #print(grad, computed_grad)\n",
    "    f = abs(grad - computed_grad)\n",
    "    m = max(abs(grad), abs(computed_grad))\n",
    "\n",
    "    if f > 0.01 and m > 0.:\n",
    "        f /= m\n",
    "\n",
    "    if f > 0.01 or math.isnan(f):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def check_gradient(function, weights, true_grad, alpha = 1e-3):\n",
    "    # because input can be of any dimension,\n",
    "    # we build a view of the underlying data with the .shape(-1) method\n",
    "    # then we can access any element of the tensor as a elements of a list\n",
    "    # with a single dimension\n",
    "    weights_view = weights.reshape(-1)\n",
    "    true_grad_view = true_grad.reshape(-1)\n",
    "    for i in range(weights_view.shape[0]):\n",
    "        old = weights_view[i]\n",
    "\n",
    "        weights_view[i] = old - alpha\n",
    "        value_left = function(weights).reshape(-1)\n",
    "\n",
    "        weights_view[i] = old + alpha\n",
    "        value_right = function(weights).reshape(-1)\n",
    "\n",
    "        weights_view[i] = old\n",
    "        grad = (value_right - value_left) / (2. * alpha)\n",
    "\n",
    "        if not is_almost_equal(grad, true_grad_view[i]):\n",
    "            return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e74c83-d581-4499-acb5-6b6ac57de0f7",
   "metadata": {},
   "source": [
    "# 2. Parameter initialization\n",
    "\n",
    "We are now going to build the function that will be used to initialize the parameters of the neural network before training.\n",
    "Note that for parameter initialization you must use **in-place** operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b81523ec-c37d-4061-ad8c-59956121eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random ndarray\n",
    "a = np.random.uniform(-1, 1, (5,))\n",
    "\n",
    "# this does not change the data of the ndarray created above!\n",
    "# it creates a new ndarray and replace the reference stored in a\n",
    "a = np.zeros((5, ))\n",
    "\n",
    "# this will change the underlying data of the ndarray that a points to\n",
    "a[:] = 0\n",
    "\n",
    "# similarly, this creates a new array and change the object pointed by a\n",
    "a = a + 1\n",
    "\n",
    "# while this change the underlying data of a\n",
    "a += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc53dc-2ca1-4619-a0e2-bdce3f65ee1d",
   "metadata": {},
   "source": [
    "For an affine transformation, it is common to:\n",
    "* initialize the bias to 0\n",
    "* initialize the projection matrix with Glorot initialization (also known as Xavier initialization)\n",
    "\n",
    "The formula for Glorot initialization can be found in equation 16 (page 5) of the original paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "**Question 6:** Fill the two initilization functions below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc85798",
   "metadata": {},
   "source": [
    "#### Weights Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82063f5e",
   "metadata": {},
   "source": [
    "- `Xavier initialization`, also known as Glorot initialization, is a method of initializing the weights of a neural network to help with the convergence during training. It is named after Xavier Glorot, who, along with `Yoshua Bengio`, introduced the method in a paper in 2010 to address the problems of training deep neural networks.\n",
    "\n",
    "- The `key` idea behind Xavier initialization is to initialize the weights in such a way that the variance of the outputs of each layer is the same as the variance of its inputs. This helps prevent the gradients from becoming too small or too large, a problem that can lead to either vanishing gradients or exploding gradients, respectively.\n",
    "\n",
    "- The Xavier initialization sets a layer's weights $W$ using a random distribution that has a zero mean and a specific variance that depends on the number of input and output units of the layer:\n",
    "\n",
    "$$ \\text{Var}(W) = \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}$$\n",
    "\n",
    "Where:\n",
    "- $n_{\\text{in}}$ is the number of units (neurons) in the layer's input (i.e., the size of the previous layer).\n",
    "- $n_{\\text{out}}$ is the number of units in the layer's output (i.e., the size of the current layer).\n",
    "\n",
    "1. For the `uniform distribution`, the weights are initialized using the following range:\n",
    "\n",
    "$$ W \\sim \\text{Uniform}\\left(-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right) $$\n",
    "\n",
    "- We will use this one for our Neural Network.\n",
    "\n",
    "2. For the `normal distribution`, they are initialized using:\n",
    "\n",
    "$$ W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}}\\right) $$\n",
    "\n",
    "In practice, when using libraries like TensorFlow or PyTorch, Xavier initialization can often be specified by using the appropriate initializer when constructing layers of a neural network. For example with TensorFlow, we could have used `tf.keras.initializers.GlorotUniform()` for uniform Xavier initialization or `tf.keras.initializers.GlorotNormal()` for normal Xavier initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad3516",
   "metadata": {},
   "source": [
    "The Bias vector is set to 0 due to 2 main reasons:\n",
    "\n",
    "- Kickstarting Learning: We can think of the bias as a starting point for the neurons. Setting the bias to zero is like starting from a neutral position, allowing the weights (which are initialized using the Glorot method) to take the lead in learning from the data. This way, each neuron begins learning without any initial push in a particular direction, making it easier for the network to adjust and learn the patterns in the data effectively.\n",
    "\n",
    "- Keeping Things Balanced: By starting the biases at zero, we help keep the outputs of neurons initially centered around zero. This balance is crucial because it helps avoid extreme values in the neurons' outputs at the beginning of training, making it smoother for the network to learn and adjust as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5171ab4-848a-47f0-92db-81acb31aeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(b):\n",
    "    return np.zeros(b).reshape(-1,1)\n",
    "\n",
    "def glorot_init(W):\n",
    "    return np.random.uniform(-np.sqrt(6.0/(W[0]+W[1])), np.sqrt(6.0/(W[0]+W[1])),W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1aede1-96af-4f2c-97ec-273b95a256c8",
   "metadata": {},
   "source": [
    "# 3. Building and training the neural network\n",
    "\n",
    "In our simple example, creating the neural network is simply instantiating the parameters $W$ and $b$.\n",
    "They must be ndarray object with the correct dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc4a37-d975-45cb-b225-f22ab642ca4b",
   "metadata": {},
   "source": [
    "**Question 7:** Fill the function that create and initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fad7abf3-fe48-4738-9eb3-2b716cb11cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters(dim_input, dim_output):\n",
    "    W = glorot_init((dim_output, dim_input))\n",
    "    b =  zero_init(dim_output)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b83f22-cc7d-4c05-9607-8d8c809558cc",
   "metadata": {},
   "source": [
    "The recent success of deep learning is (partly) due to the ability to train very big neural networks.\n",
    "However, researchers became interested in building small neural networks to improve computational efficiency and memory usage.\n",
    "Therefore, we often want to compare neural networks by their number of parameters, i.e. the size of the memory required to store the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e4375-f265-4bb2-9f37-889713f7e277",
   "metadata": {},
   "source": [
    "**Question 8:** Fill the function that  print the number of parameters of the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7729a857-04ab-4a7a-b85c-cf171b62501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_parameters(W, b):\n",
    "    # If W is (5,3) then W.size = 15\n",
    "    n = W.size + b.size\n",
    "    print(\"Number of parameters: %i\" % (n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce568ce-415c-4750-9169-5c67dc9653cf",
   "metadata": {},
   "source": [
    "We can now create the neural network and print its number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8f54c2a-98a1-4c06-94dc-511b05cd7bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 60\n"
     ]
    }
   ],
   "source": [
    "dim_input = 5\n",
    "dim_output = 10\n",
    "W, b = create_parameters(dim_input, dim_output)\n",
    "print_n_parameters(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f7f13-4f6d-4c03-a9bf-5776571dec58",
   "metadata": {},
   "source": [
    "Finally, the training loop!\n",
    "\n",
    "The training loop should be structured as follows:\n",
    "* We do **epochs** over the data, i.e. one epoch is one loop over the dataset\n",
    "* At each epoch, we first loop over the data and update the network parameters with respect to the loss gradient\n",
    "* At the end of each epoch, we evaluate the network on the dev dataset\n",
    "* After all epochs are done, we evaluate our network on the test dataset and compare its performance with the performance on dev (development = validation) dataset\n",
    "\n",
    "During training, it is useful to print the following information:\n",
    "* The $\\textcolor{red}{\\text{mean loss}}$ over the epoch: it should be $\\textcolor{red}{\\text{decreasing}}$!\n",
    "* The $\\textcolor{green}{\\text{accuracy on the dev/val set}}$: it should be $\\textcolor{green}{\\text{increasing}}$!\n",
    "* The $\\textcolor{green}{\\text{accuracy on the train set}}$: it shoud be $\\textcolor{green}{\\text{increasing}}$!\n",
    "\n",
    "If you observe a $\\textcolor{orange}{\\text{decreasing loss}}$ (+ $\\textcolor{orange}{\\text{increasing accuracy on test data}}$) but $\\textcolor{orange}{\\text{decreasing accuracy on dev data}}$, your network is $\\textcolor{orange}{\\text{overfitting}}$!\n",
    "\n",
    "Once you have built **and tested** this simple training loop, you should introduce the following improvements:\n",
    "* instead of evaluating on dev after each loop on the training data, you can also evaluate on dev n times per epoch\n",
    "* shuffle the data before each epoch\n",
    "* instead of memorizing the parameters of the last epoch only, you should have a copy of the parameters that produced the best value on dev data during training and evaluate on test with those instead of the parameters after the last epoch\n",
    "* learning rate decay: if you do not observe improvement on dev, you can try to reduce the step size\n",
    "\n",
    "After you conducted (successful?) experiments, you should write a report with results (in the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4a3d1",
   "metadata": {},
   "source": [
    "#### Metric: Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea6b3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W, b, X, y):\n",
    "    logits = affine_transform(W, b, X)\n",
    "    proprobabilities = softmax(logits)\n",
    "    predictions = np.argmax(proprobabilities, axis=0)\n",
    "    return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8bb2e",
   "metadata": {},
   "source": [
    "#### Parameter Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84d339de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training, we initialize the parameters of the network\n",
    "n_training, dim_input, dim_output = getDimDataset(train_data)\n",
    "\n",
    "# Initialization of Weights, Bias, n_epochs, learning_rate and two indicators (best_dev_acc and best_params)\n",
    "\n",
    "# Weights and Bias\n",
    "W1, b1 = create_parameters(dim_input, dim_output)\n",
    "\n",
    "# Number of epochs\n",
    "n_epochs = 20\n",
    "\n",
    "# Setting initial learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Defining decay factor\n",
    "#decay_factor = 0.01\n",
    "\n",
    "# Record the best development accuracy\n",
    "best_dev_acc = -np.inf\n",
    "\n",
    "# Record the best model parameters\n",
    "best_params = (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55b5b6ae-43be-4017-8a19-9cbd122302f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: [0.95649239], Train acc: 0.89102, Dev acc: 0.8982\n",
      "Epoch 2, Loss: [0.90366951], Train acc: 0.86954, Dev acc: 0.871\n",
      "Epoch 3, Loss: [0.88883658], Train acc: 0.8884, Dev acc: 0.89\n",
      "Epoch 4, Loss: [0.86683821], Train acc: 0.8903, Dev acc: 0.8884\n",
      "Epoch 5, Loss: [0.86061895], Train acc: 0.89536, Dev acc: 0.8967\n",
      "Epoch 6, Loss: [0.84688754], Train acc: 0.84832, Dev acc: 0.84\n",
      "Epoch 7, Loss: [0.84788048], Train acc: 0.89108, Dev acc: 0.8933\n",
      "Epoch 8, Loss: [0.83493164], Train acc: 0.88096, Dev acc: 0.8819\n",
      "Epoch 9, Loss: [0.84383148], Train acc: 0.91476, Dev acc: 0.9117\n",
      "Epoch 10, Loss: [0.83056658], Train acc: 0.91176, Dev acc: 0.9087\n",
      "Epoch 11, Loss: [0.83295227], Train acc: 0.91318, Dev acc: 0.9075\n",
      "Epoch 12, Loss: [0.83107542], Train acc: 0.87114, Dev acc: 0.8683\n",
      "Epoch 13, Loss: [0.83315779], Train acc: 0.90084, Dev acc: 0.8937\n",
      "Epoch 14, Loss: [0.82656243], Train acc: 0.90262, Dev acc: 0.8978\n",
      "Epoch 15, Loss: [0.83221275], Train acc: 0.89202, Dev acc: 0.8932\n",
      "Epoch 16, Loss: [0.8283714], Train acc: 0.89552, Dev acc: 0.8941\n",
      "Epoch 17, Loss: [0.82903749], Train acc: 0.89052, Dev acc: 0.8862\n",
      "Epoch 18, Loss: [0.81452997], Train acc: 0.90198, Dev acc: 0.8969\n",
      "Epoch 19, Loss: [0.81328986], Train acc: 0.90254, Dev acc: 0.8964\n",
      "Epoch 20, Loss: [0.81770788], Train acc: 0.88678, Dev acc: 0.8772\n",
      "Test acc: 0.9126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle the training data to prevent cycles\n",
    "    X_train, y_train = shuffle(train_data[0], train_data[1])\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        \n",
    "        x = x.reshape(1, -1)         \n",
    "        \n",
    "        # Forward pass and loss computation\n",
    "        logits = affine_transform(W1, b1, x)\n",
    "        \n",
    "        # Negative Log-Likelihood Loss\n",
    "        loss = nll(logits, y)\n",
    "\n",
    "        # Epoch Loss Accumulator\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Backward pass\n",
    "        grad_logits = backward_nll(logits, y, 1.0)\n",
    "\n",
    "        grad_W, grad_b = backward_affine_transform(W1, b1, x, grad_logits)\n",
    "\n",
    "        # Parameters update\n",
    "        W1 = W1 - learning_rate * grad_W\n",
    "        b1 = b1 - learning_rate * grad_b\n",
    "    \n",
    "    # Computing the average loss\n",
    "    avg_loss = epoch_loss / len(X_train)\n",
    "    \n",
    "    # Evaluate the model on the training and development set\n",
    "    train_acc = accuracy(W1, b1, train_data[0], train_data[1])\n",
    "    dev_acc = accuracy(W1, b1, dev_data[0], dev_data[1])\n",
    "    \n",
    "    # Print the performance\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}, Train acc: {train_acc}, Dev acc: {dev_acc}')\n",
    "    \n",
    "    # If the model is the best, we update the best model parameters\n",
    "    if dev_acc > best_dev_acc:\n",
    "        best_dev_acc = dev_acc\n",
    "        best_params = (W1.copy(), b1.copy())\n",
    "\n",
    "# Use the best model parameters to evaluate the test set\n",
    "W1, b1 = best_params\n",
    "test_acc = accuracy(W1, b1, test_data[0], test_data[1])\n",
    "print(f'Test acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e80ebe",
   "metadata": {},
   "source": [
    "### Final Report on MNIST Classification Results\n",
    "\n",
    "**Model Overview:**\n",
    "The model employed is a single-layer neural network utilizing an affine transformation followed by a softmax output. We have trained the model using the Negative Log-Likelihood (NLL) as the loss function and have implemented a basic Stochastic Gradient Descent (SGD) for optimization.\n",
    "\n",
    "**Dataset:**\n",
    "Training was conducted on the MNIST dataset, a standard benchmark comprising 28x28 pixel grayscale images of handwritten digits, labeled 0 through 9.\n",
    "\n",
    "**Training Process:**\n",
    "Training proceeded for 20 epochs, each involving a shuffle of the training data to ensure randomness in the mini-batch SGD process. The data was reshaped to fit the model, and a forward pass, loss calculation, backward pass, and parameter update sequence were executed iteratively for each data point.\n",
    "\n",
    "**Results Summary:**\n",
    "- Loss Analysis: The loss showed a general decreasing trend over 20 epochs, starting from 0.9565 to 0.8133. This indicates that the model was learning and improving its predictions over time, adjusting its parameters to minimize the error between the predicted and true labels.\n",
    "\n",
    "- Training Accuracy Trends: Training accuracy experienced some fluctuations throughout the epochs. It began at 89.102% and saw variations, with a notable dip to 84.832% at epoch 6 and peaking at 91.476% by epoch 9. The training accuracy closed at 88.678% in epoch 20. These fluctuations suggest that the model's performance on the training set varied significantly, which could be due to the learning rate adjustments or the inherent stochastic nature of the training process.\n",
    "\n",
    "- Development (Validation) Accuracy Movements: Similar to training accuracy, development accuracy also fluctuated. It started at 89.82%, dropped to 84% at epoch 6, reached its highest at 91.17% by epoch 9, and ended at 87.72% by epoch 20. The peaks and troughs in validation accuracy indicate that the model's generalization to unseen data varied significantly throughout the training process, which could be reflective of overfitting to the training data in certain epochs or possibly the impact of learning rate adjustments.\n",
    "\n",
    "- Final Test Accuracy: The model achieved a test accuracy of 91.26% on unseen data. This is a strong result, suggesting that despite the fluctuations in training and development accuracy, the model was able to generalize well to new data. The test accuracy being in close range to the highest development accuracy indicates a balanced model without significant overfitting.\n",
    "\n",
    "**Observations:**\n",
    "- The development set's accuracy fluctuated above and below the training accuracy, suggesting that the model may be on the verge of overfitting, as indicated by the highest dev accuracy not occurring at the last epoch.\n",
    "- The development accuracy, while generally improving, did not exhibit a clear upward trend. This could suggest the model might benefit from more complex architectures or regularization techniques.\n",
    "\n",
    "**Recommendations for Future Improvements:**\n",
    "\n",
    "1. **Regularization**: We could introduce L1 or L2 regularization to penalize large weights and mitigate overfitting.\n",
    "2. **Learning Rate Schedule**: We could implement adaptive learning rates, like learning rate decay or advanced optimizers (e.g., Adam), which can help converge faster and possibly reach better local minima.\n",
    "3. **Model Complexity**: We could introduce more layers (a deeper network) or hidden units (a wider network) to capture more complex features, potentially increasing accuracy.\n",
    "4. **Early Stopping**: We could monitor the development accuracy and stop training when it no longer improves, using the best model on the development set for the final evaluation.\n",
    "5. **Data Augmentation**: We could employ techniques such as rotation, translation, or scaling to artificially expand the training dataset and improve generalization.\n",
    "6. **Batch Processing**: We could, instead of stochastic updates, use mini-batch gradient descent to take advantage of vectorized operations for faster processing.\n",
    "7. **Hyperparameter Tuning**: Systematically search for the best hyperparameters, including the learning rate and batch size, possibly employing grid search or random search methods.\n",
    "8. **Dropout**: We could introduce dropout layers, mentioned in class, to prevent co-adaptation of neurons and encourage individual feature detection.\n",
    "9. **Cross-validation**: We could use k-fold cross-validation to make better use of the development data and achieve a more robust estimation of model performance.\n",
    "\n",
    "**Conclusion:**\n",
    "The implemented model demonstrates decent classification performance on the MNIST dataset. However, there is room for improvement, particularly in addressing the potential for overfitting and exploring more advanced model architectures or training schemes to further boost accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
