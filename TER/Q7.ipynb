{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION OF DATASET WHICH CONTAINS\n",
    "# --------------------------------------------------------\n",
    "#  ID_Citing | Category_Citing | Claims_Text                                      | ID_Cited      | References_Cited                                          | Content_Cited # Varying field depending on sample\n",
    "# \"3672233A1\"  \"X\"               [{Claim 1: Content}, {Claim 2: Content}, ...]      \"2709350A1\"     '* abstract ** paragraphs [0065] - [0073] ** figure 6 *'    {pa01: Content, pnum: Content, fnum: Content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "from html import unescape\n",
    "\n",
    "# 04/03\n",
    "# Task 2:\n",
    "# Solve case \"THE WHOLE DOCUMENT\" (Extraction of only necessary info - PAGES + PARAGRAPHS + ABSTRACT + FIGURES + ¿LINES?): 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 14, 15 \n",
    "# Count how many A,X,Y\n",
    "# given 1000 pairs of documents (citing, cited)\n",
    "# find the paragraphs of cited that were pointed out by the examiner\n",
    "# and give the category, A or X\n",
    "\n",
    "# Task 2:\n",
    "                                                        #{\n",
    "#(p20201, p20101) -> rank of cited paragraphs {\n",
    "                                                # average\n",
    "#(p20201, p20102) -> rank of cited paragraphs }\n",
    "                                                            #average\n",
    "#(p20202, p20103) -> rank of cited paragraphs\n",
    "                                                        #}\n",
    "\n",
    "\n",
    "# GOAL: Train & Test Dataset\n",
    "# Take a look at the RAG DOC\n",
    "# 5/04 PRESENTATION\n",
    "\n",
    "# Pavlo and Pablo -\n",
    "def cleaner_cited_references(cited_ID, paragraphs, json_cited):\n",
    "    dict_content = {}\n",
    "    dict_content2 = {}\n",
    "    \n",
    "    matching_entry = next(\n",
    "        (element for element in json_cited if element[\"application_number\"] + element[\"application_category\"] == cited_ID), None\n",
    "    )\n",
    "    \n",
    "\n",
    "    if matching_entry:\n",
    "        fig_list = []\n",
    "\n",
    "        # Cases:\n",
    "        # Case 1. paragraphs = \"\"\n",
    "        # Case 2. paragraphs = \" the whole document\"\n",
    "        # Case 3. paragraphs = \" the whole document * figure 3\"\n",
    "        #print(\"\\n------------\", matching_entry[\"application_number\"], \"-------------\",\"\\n\")\n",
    "        #print(\"Paragraphs:\", paragraphs)\n",
    "        if not paragraphs or 'the whole document' in paragraphs.lower():\n",
    "            \n",
    "            # DONE(except multiple cases)\n",
    "            # Case 2 or 3\n",
    "            \n",
    "            if paragraphs:\n",
    "                paragraphs = paragraphs.replace(\"* the whole document *\", \"\")\n",
    "                if not paragraphs:\n",
    "                    return dict_content\n",
    "                else:\n",
    "                    \n",
    "                    if \"abstract\" in paragraphs.lower() and \"abstract\" in matching_entry:\n",
    "                        abstract_clean = re.sub(r'<[^>]+>', '', matching_entry[\"abstract\"]).strip()[2:]\n",
    "                        dict_content.update({\"pa01\":abstract_clean})\n",
    "                    \n",
    "                    # Extraction of Paragraphs IDs\n",
    "                    paragraphs_IDs = extract_paragraph_numbers_the_whole_document(paragraphs)\n",
    "                    # List of IDs not empty\n",
    "                    #print(\"Paragraphs IDs - with whole doc:\", paragraphs_IDs)\n",
    "                    if paragraphs_IDs:\n",
    "                        dict_paragraphs_content = extract_paragraphs_by_ids(matching_entry[\"description\"], paragraphs_IDs)\n",
    "                        #print(\"Paragraphs IDs Content - with whole doc:\", dict_paragraphs_content)\n",
    "                        dict_content.update(dict_paragraphs_content)\n",
    "                        #dict_content2[\"Paragraphs_Text\"] = list_paragraphs_content\n",
    "\n",
    "                    # Extraction of Figure IDs\n",
    "\n",
    "                    figure_IDs = extract_figure_ids_per_line(paragraphs)\n",
    "                    #print(\"FIGURE IDS\", figure_IDs)\n",
    "\n",
    "                    figures_info = extraction_paragraphs_with_figures(matching_entry[\"description\"], figure_IDs)\n",
    "                    #print(\"FIGURES INFO\", figures_info)\n",
    "                    #print(figures_info)\n",
    "                    dict_content.update(figures_info)\n",
    "                    # Extraction of Figures IDs + Content\n",
    "\n",
    "                    # figures_to_extract = extract_figure_ids_per_line(paragraphs)\n",
    "                    # #print(figures_to_extract)\n",
    "                    # content_all_figures = extract_lines_with_figure_references(matching_entry[\"description\"])\n",
    "                    #print(content_all_figures)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    # Extraction of specific Claim IDs + Content\n",
    "\n",
    "                    # Extraction of Claims IDs - If not mentioned, returns empty list\n",
    "                    claims_IDs_to_filter = extract_claims_from_paragraphs(paragraphs)\n",
    "                    #print(claims_IDs_to_filter)\n",
    "                    # If claims_IDs_to_filter == [\"s\"], return all claims\n",
    "                    if claims_IDs_to_filter == [0]:\n",
    "                        dict_content.update(extract_claim_info(json_cited, cited_ID))\n",
    "                    if claims_IDs_to_filter:\n",
    "                        # Extraction of content of Claims\n",
    "                        dict_claims_content = extract_claim_info(json_cited, cited_ID, claims_IDs_to_filter)\n",
    "                        # print(dict_claims_content)\n",
    "                        # Matching specific IDs from dict_claims_content and claims_IDs_to_filter\n",
    "                        # dict_claims_content_filtered = {key: dict_claims_content[key] for key in claims_IDs_to_filter}\n",
    "                        #dict_claims_content_filtered = [claim for claim in dict_claims_content if any(key in claim for key in claims_IDs_to_filter)]\n",
    "                        dict_content.update(dict_claims_content)\n",
    "\n",
    "\n",
    "                    \n",
    "                    return dict_content\n",
    "\n",
    "                # # Extraction of complete Abstract\n",
    "                # abstract_clean = re.sub(r'<[^>]+>', '', matching_entry[\"abstract\"]).strip()\n",
    "                # dict_content[\"pa01\"] = abstract_clean\n",
    "                \n",
    "                # # Extraction of complete Description\n",
    "                # description_clean = re.sub(r'<[^>]+>', '', matching_entry[\"description\"]).strip()\n",
    "                # dict_paragraphs_content_w = extract_paragraphs_by_ids(matching_entry[\"description\"])\n",
    "                # dict_content.update(dict_paragraphs_content_w)\n",
    "\n",
    "                # # Extraction of all Claims content\n",
    "                # dict_content.update(extract_claim_info(json_cited, cited_ID))\n",
    "                # #dict_content[\"Claims\"] = extract_claim_info(json_cited, cited_ID)\n",
    "\n",
    "                # # Case 3\n",
    "\n",
    "                # # Extraction of Specific Figure(s) within Complete                                                                     \n",
    "                # figures_to_extract = extract_figure_ids_per_line(paragraphs)\n",
    "                # #print(figures_to_extract)\n",
    "                # content_all_figures = extract_lines_with_figure_references(matching_entry[\"description\"])\n",
    "                # #print(content_all_figures)\n",
    "                # # We look for all references keywords = [\"Figure\", \"Figures\", \"FIG.\", \"FIGS.\", \"Fig.\", \"Figs.\"] within the complete extracted content and search\n",
    "                \n",
    "                \n",
    "\n",
    "                # return dict_content                                                                                                    \n",
    "            \n",
    "            #DONE\n",
    "            # Case 1\n",
    "            else: # We don't need to include all the info, leave empty don't skip the cited doc\n",
    "                # Extraction of complete Abstract\n",
    "                #abstract_clean = re.sub(r'<[^>]+>', '', matching_entry[\"abstract\"]).strip()[3:]\n",
    "                # We don't include it\n",
    "                #dict_content[\"pa01\"] = abstract_clean\n",
    "                \n",
    "                # Extraction of complete Description\n",
    "                #description_clean = re.sub(r'<[^>]+>', '', matching_entry[\"description\"]).strip()\n",
    "                # We don't include it\n",
    "                #dict_content[\"Description\"] = description_clean\n",
    "\n",
    "                # Extraction of all Claims content \n",
    "                # We don't include it\n",
    "                #dict_content.update(extract_claim_info(json_cited, cited_ID))\n",
    "                \n",
    "                # Extraction of all Figure Description content\n",
    "                # We don't include it - Not done\n",
    "                #figures_to_extract = extract_figure_ids_per_line(paragraphs)\n",
    "                #print(figures_to_extract)\n",
    "                #content_all_figures = extract_lines_with_figure_references(matching_entry[\"description\"])\n",
    "                #print(content_all_figures)\n",
    "                #dict_content[\"Figure Description\"] = content_all_figures\n",
    "                return dict_content\n",
    "\n",
    "        else:\n",
    "            # \"* abstract ** paragraph [0016] - paragraph [0017] ** paragraph [0023] ** paragraph [0034] - paragraph [0044]; figure 3 *\n",
    "            # \"* claims 1-12; figures 1-4 *\"\n",
    "            \n",
    "\n",
    "\n",
    "            # Extraction of complete Abstract\n",
    "            #print(\"PARAGRAPHS\", paragraphs)\n",
    "            #print(\"MATCHING ENTRY\", matching_entry)\n",
    "            # DONE\n",
    "            if \"abstract\" in paragraphs.lower() and \"abstract\" in matching_entry:\n",
    "                #print(paragraphs)\n",
    "                #print(matching_entry)\n",
    "                #print(matching_entry[\"abstract\"])\n",
    "                abstract_clean = re.sub(r'<[^>]+>', '', matching_entry[\"abstract\"]).strip()[2:]\n",
    "                dict_content2.update({\"pa01\":abstract_clean})\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # Extraction of Paragraphs IDs + Content from Description\n",
    "            \n",
    "            # Extraction of Paragraphs IDs\n",
    "            paragraphs_IDs = extract_and_expand_numbers(paragraphs)\n",
    "            #print(\"Paragraphs IDs - WITHOUT whole doc:\", paragraphs_IDs)\n",
    "            #print(\"PARAGRAPHS IDs\", paragraphs_IDs)\n",
    "            # List of IDs not empty\n",
    "            if paragraphs_IDs:\n",
    "                dict_paragraphs_content = extract_paragraphs_by_ids(matching_entry[\"description\"], paragraphs_IDs)\n",
    "                #print(\"PARAGRAPHS INFO\", dict_paragraphs_content)\n",
    "                #print(\"Paragraphs IDs Content - WITHOUT whole doc:\", dict_paragraphs_content)\n",
    "                dict_content2.update(dict_paragraphs_content)\n",
    "                #dict_content2[\"Paragraphs_Text\"] = list_paragraphs_content\n",
    "\n",
    "\n",
    "            # Extraction of Figure IDs\n",
    "\n",
    "            figure_IDs = extract_figure_ids_per_line(paragraphs)\n",
    "            #print(\"FiGURE IDs\", figure_IDs)\n",
    "            #print(figure_IDs)\n",
    "            # List of IDs not empty\n",
    "            #if figure_IDs:\n",
    "                #list_figures_content =                                                                                                 # TO DO   \n",
    "                #dict_content[\"Figures_Text\"] = figure_IDs\n",
    "            figures_info = extraction_paragraphs_with_figures(matching_entry[\"description\"], figure_IDs)\n",
    "            #print(\"FIGURE INFO\", figures_info)\n",
    "            #print(figures_info)\n",
    "            dict_content2.update(figures_info)\n",
    "            # Extraction of Figures IDs + Content\n",
    "\n",
    "            # figures_to_extract = extract_figure_ids_per_line(paragraphs)\n",
    "            # #print(figures_to_extract)\n",
    "            # content_all_figures = extract_lines_with_figure_references(matching_entry[\"description\"])\n",
    "            #print(content_all_figures)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            # Extraction of specific Claim IDs + Content\n",
    "\n",
    "            # Extraction of Claims IDs - If not mentioned, returns empty list\n",
    "            claims_IDs_to_filter = extract_claims_from_paragraphs(paragraphs)\n",
    "            #print(claims_IDs_to_filter)\n",
    "            # If claims_IDs_to_filter == [\"s\"], return all claims\n",
    "            if claims_IDs_to_filter == [0]:\n",
    "                dict_content2.update(extract_claim_info(json_cited, cited_ID))\n",
    "            if claims_IDs_to_filter:\n",
    "                # Extraction of content of Claims\n",
    "                dict_claims_content = extract_claim_info(json_cited, cited_ID, claims_IDs_to_filter)\n",
    "                # print(dict_claims_content)\n",
    "                # Matching specific IDs from dict_claims_content and claims_IDs_to_filter\n",
    "                # dict_claims_content_filtered = {key: dict_claims_content[key] for key in claims_IDs_to_filter}\n",
    "                #dict_claims_content_filtered = [claim for claim in dict_claims_content if any(key in claim for key in claims_IDs_to_filter)]\n",
    "                dict_content2.update(dict_claims_content)\n",
    "\n",
    "\n",
    "            \n",
    "            return dict_content2\n",
    "\n",
    "# Pavlo\n",
    "def format_figure_references(text):\n",
    "    # Regular expression pattern to match 'figures' followed by the references\n",
    "    pattern = r'(figures?\\s*[\\d\\w-]+(?:,\\s*[\\d\\w-]+)*)'\n",
    "\n",
    "    def replace_commas(match):\n",
    "        # Replace commas followed by spaces with just a comma in the matched text\n",
    "        match_text = re.sub(r',\\s+', ',', match.group(0))\n",
    "        # Correctly handle ranges and individual figures\n",
    "        match_text = re.sub(r'(\\d+)-(\\d+|\\w*)', lambda m: m.group(1) if not m.group(2) else m.group(0), match_text)\n",
    "        match_text = re.sub(r'(\\d+)-\\d+-(\\d+)', r'\\1-\\2', match_text)\n",
    "        return match_text\n",
    "\n",
    "    # Apply the replacement function to each match of the pattern in the text\n",
    "    formatted_text = re.sub(pattern, replace_commas, text)\n",
    "    return formatted_text\n",
    "\n",
    "# Pavlo\n",
    "def separate_alpha_numeric(text):\n",
    "    numeric_part = ''.join(filter(str.isdigit, text))\n",
    "    alphabetic_part = ''.join(filter(str.isalpha, text))\n",
    "    return numeric_part, alphabetic_part\n",
    "\n",
    "# Pavlo\n",
    "def custom_sort_key(fig_id):\n",
    "    try:\n",
    "        numeric_part, alpha_part = separate_alpha_numeric(fig_id)\n",
    "        numeric_value = int(numeric_part) if numeric_part else 0\n",
    "        alpha_value = ord(alpha_part) if alpha_part else 0  # Sort by alphabetic part if present\n",
    "        return numeric_value, alpha_value\n",
    "    except Exception as e:\n",
    "        return 0, 0\n",
    "\n",
    "# Pavlo\n",
    "def expand_range(start_fig, end_fig):\n",
    "    start_num_part, start_alpha_part = separate_alpha_numeric(start_fig)\n",
    "    end_num_part, end_alpha_part = separate_alpha_numeric(end_fig)\n",
    "\n",
    "    if start_num_part == end_num_part and start_alpha_part and end_alpha_part:\n",
    "        # Handle alphanumeric range, e.g., '3a' to '3c'\n",
    "        start_index = ord(start_alpha_part)\n",
    "        end_index = ord(end_alpha_part)\n",
    "        return [f\"{start_num_part}{chr(alpha)}\" for alpha in range(start_index, end_index + 1)]\n",
    "    elif not start_alpha_part and not end_alpha_part:\n",
    "        # Handle numeric range, e.g., '1' to '3'\n",
    "        return [str(i) for i in range(int(start_num_part), int(end_num_part) + 1)]\n",
    "    else:\n",
    "        # If the range is mixed or invalid, return an empty list\n",
    "        return []\n",
    "\n",
    "def extract_figure_ids_per_line(text):\n",
    "    # First, format the figure references in the text\n",
    "    formatted_text = format_figure_references(text)\n",
    "\n",
    "    # Check if the text mentions \"figures\" without specifying any numbers\n",
    "    if re.search(r'\\bfigures?\\b', formatted_text, re.IGNORECASE) and not re.search(r'\\bfigures?\\s+\\d', formatted_text, re.IGNORECASE):\n",
    "        return [0]\n",
    "\n",
    "    # Then, extract all figure IDs from the formatted text\n",
    "    figure_matches = re.findall(r'figures?\\s*([0-9a-z,-]+(?:,\\s*\\d+(?:-\\d+)?)*)', formatted_text, re.IGNORECASE)\n",
    "    pattern = re.compile(r'\\d+[A-Za-z]-\\d+')\n",
    "    figure_matches = [match for match in figure_matches if not pattern.search(match)]\n",
    "    all_figure_ids = set()\n",
    "\n",
    "    try:\n",
    "        for match in figure_matches:\n",
    "            figures_list = match.split(',')\n",
    "            for fig in figures_list:\n",
    "                fig = fig.strip()\n",
    "                if '-' in fig:\n",
    "                    start_fig, end_fig = fig.split('-')\n",
    "                    all_figure_ids.update(expand_range(start_fig, end_fig))\n",
    "                elif fig:\n",
    "                    all_figure_ids.add(fig)\n",
    "    except Exception as e:\n",
    "        return sorted(all_figure_ids, key=custom_sort_key)\n",
    "\n",
    "    return sorted(all_figure_ids, key=custom_sort_key)\n",
    "\n",
    "# Pavlo and Pablo -\n",
    "def extract_paragraph_numbers_the_whole_document(text):\n",
    "\n",
    "    pattern_to_delete = re.compile(r'\\bparagraph\\s*\\[\\d+\\]\\s*-\\s*paragraph\\s*\\[\\d+\\]')\n",
    "    \n",
    "    # Function to adjust the matched pattern\n",
    "    def adjust_match(match):\n",
    "        # Split the matched string by \" - \"\n",
    "        parts = match.group().split(\" - \")\n",
    "        # Remove the second occurrence of \"paragraph\"\n",
    "        adjusted = parts[0] + \" - \" + parts[1].replace(\"paragraph \", \"\", 1)\n",
    "        return adjusted\n",
    "    \n",
    "    # Apply the adjustment to all matches in the text\n",
    "    adjusted_text = re.sub(pattern_to_delete, adjust_match, text)\n",
    "\n",
    "    # Normalize the text by replacing \"and\" with a comma and removing square brackets\n",
    "    normalized_text = re.sub(r'\\sand\\s', ',', adjusted_text)\n",
    "    normalized_text = re.sub(r'\\s?-\\s?', '-', normalized_text)\n",
    "    normalized_text = re.sub(r'[\\[\\]]', '', normalized_text)\n",
    "\n",
    "    # Define a regex pattern that matches paragraph references in various formats\n",
    "    pattern = re.compile(r'\\b(paragraphs|para\\.|paragraph)\\s*(\\d+(?:-\\d+)?(?:,\\s*\\d+(?:-\\d+)?)*)', re.IGNORECASE)\n",
    "\n",
    "    # Initialize an empty list to store all extracted paragraph numbers\n",
    "    numbers = []\n",
    "\n",
    "    # Find all matches and process each one\n",
    "    for match in pattern.findall(normalized_text):\n",
    "        parts = match[1].split(',')\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if '-' in part:\n",
    "                # Process ranges\n",
    "                start, end = map(int, part.split('-'))\n",
    "                numbers.extend(range(start, end + 1))\n",
    "            else:\n",
    "                # Process single numbers\n",
    "                numbers.append(int(part))\n",
    "    \n",
    "    # Return a sorted list of unique paragraph numbers\n",
    "    return sorted(set(numbers))\n",
    "\n",
    "# Pablo --\n",
    "def extract_and_expand_numbers(text):\n",
    "    pattern = re.compile(r\"\\[(\\d{4})\\](?:\\s*-\\s*paragraph\\s*\\[(\\d{4})\\]|\\s*-\\s*\\[(\\d{4})\\])?\")\n",
    "    matches = pattern.findall(text)\n",
    "    numbers = []\n",
    "    for match in matches:\n",
    "        start = match[0]\n",
    "        end = match[1] if match[1] else match[2]\n",
    "        if start and end:\n",
    "            start_num, end_num = int(start), int(end)\n",
    "            numbers.extend(range(start_num, end_num + 1))\n",
    "        elif start:\n",
    "            numbers.append(int(start))\n",
    "    return sorted(set(numbers))\n",
    "\n",
    "def clean_text(html_text):\n",
    "    clean_text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "# Pablo--\n",
    "def extract_paragraphs_by_ids(text, paragraph_ids=None):\n",
    "    extracted_paragraphs = {}\n",
    "    # Update to match any paragraph ID if None provided\n",
    "    if paragraph_ids:\n",
    "        for pid in paragraph_ids:\n",
    "            pattern = re.compile(r'<p id=\"p{:04d}\" num=\"\\d+\">(.*?)</p>'.format(int(pid)), re.DOTALL)\n",
    "            match = pattern.search(text)\n",
    "            if match:\n",
    "                paragraph_text = match.group(1)\n",
    "                paragraph_text = re.sub(r'<figref idref=\"[^\"]+\">(.*?)</figref>', r'\\1', paragraph_text)\n",
    "                paragraph_text = re.sub(r'<[^>]+>', '', paragraph_text)\n",
    "                extracted_paragraphs[f\"p{pid:04d}\"] = paragraph_text\n",
    "    else:\n",
    "        paragraph_pattern = re.compile(r'<p id=\"p(\\d+)\"[^>]*>(.*?)</p>', re.DOTALL)\n",
    "        matches = paragraph_pattern.findall(text)\n",
    "        for pid, pcontent in matches:\n",
    "            # Clean up <figref> tags and other HTML tags from paragraphs\n",
    "            pcontent_clean = re.sub(r'<figref idref=\"[^\"]+\">(.*?)</figref>', r'\\1', pcontent)\n",
    "            pcontent_clean = re.sub(r'<[^>]+>', '', pcontent_clean).strip()\n",
    "            extracted_paragraphs[f\"p{int(pid):04d}\"] = pcontent_clean\n",
    "    return extracted_paragraphs\n",
    "\n",
    "# Pablo--\n",
    "def extract_claims_from_paragraphs(text):\n",
    "\n",
    "    claim_numbers = []\n",
    "\n",
    "    # Check if \"claims\" is mentioned without specific numbers\n",
    "    if re.search(r'\\bclaims?;|\\bclaims?$', text, re.IGNORECASE):\n",
    "        return [0]\n",
    "\n",
    "    # Pattern to find claims and their ranges, correcting for spaces around dashes\n",
    "    claims_pattern = re.compile(r'\\bclaims?\\s*([0-9,-]+)', re.IGNORECASE)\n",
    "    text = re.sub(r'(\\d+)\\s*-\\s*(\\d+)', r'\\1-\\2', text)  # Remove spaces around dashes in ranges\n",
    "    text = re.sub(r'\\s*-\\s*', '-', text)  # Remove spaces around dashes in ranges\n",
    "    #print(text)\n",
    "    claims_matches = claims_pattern.findall(text)\n",
    "    \n",
    "    for match in claims_matches:\n",
    "        claim_list = match.split(',')\n",
    "        for claim_range in claim_list:\n",
    "            claim_range = claim_range.strip()\n",
    "            if '-' in claim_range:\n",
    "                parts = claim_range.split('-')\n",
    "                # Ensure both parts are valid before proceeding\n",
    "                if len(parts) == 2 and parts[0].isdigit() and parts[1].isdigit():\n",
    "                    start_claim, end_claim = map(int, parts)\n",
    "                    claim_numbers.extend(range(start_claim, end_claim + 1))\n",
    "            elif claim_range.isdigit():\n",
    "                claim_numbers.append(int(claim_range))\n",
    "\n",
    "    return sorted(set(claim_numbers))\n",
    "\n",
    "# Pavlo & Pablo --\n",
    "def extract_claim_info(json_citing, id_citing, cited_claims=None):\n",
    "    matching_entry = next(\n",
    "        (element for element in json_citing if element[\"application_number\"] + element[\"application_category\"] == id_citing), None\n",
    "    )\n",
    "    if matching_entry:\n",
    "        claim_regex = r'(?:<clai)?m id=\"c-en-(\\d+)\" num=\"(\\d+)\">(.*?)</claim>'\n",
    "        claim_text_regex = r'<claim-text>(.*?)</claim-text>'\n",
    "        # Extracting all claims from the json_mapper\n",
    "        claims = re.findall(claim_regex, matching_entry['claims'], re.DOTALL)\n",
    "        claim_info = {}\n",
    "        claim_list = []\n",
    "        \n",
    "        for claim_id, claim_num, claim_content in claims:\n",
    "            claim_list.append(int(claim_num))\n",
    "            claim_texts = re.findall(claim_text_regex, claim_content, re.DOTALL)\n",
    "            claim_text = ' '.join(claim_texts).strip()\n",
    "            claim_text = claim_text.replace(\"<claim-text>\", \" \")\n",
    "            claim_text = claim_text.replace(\"</claim-text>\", \" \")\n",
    "            claim_text = re.sub(r'<[^>]+>', '', claim_text).strip()\n",
    "            claim_info[f\"c-en-{int(claim_num):04d}\"] = claim_text\n",
    "            \n",
    "            \n",
    "        if cited_claims:\n",
    "            return {f\"c-en-{num:04d}\": claim_info.get(f\"c-en-{int(num):04d}\", \"Claim text not found\") for num in cited_claims}\n",
    "        else:\n",
    "            return {f\"c-en-{num:04d}\": claim_info.get(f\"c-en-{num:04d}\", \"Claim text not found\") for num in claim_list}\n",
    "    else:\n",
    "        return {f\"c-en-{num:04d}\": \"Claim text not found\" for num in cited_claims}\n",
    "        \n",
    "# Pablo--\n",
    "def extract_lines_with_figure_references(text):\n",
    "    # Remove HTML tags but keep their content, especially for <figref>\n",
    "    cleaned_text = re.sub(r'<figref idref=\\\"\\w+\\\">', '', text)\n",
    "    \n",
    "    # Some cleaning\n",
    "    cleaned_text = cleaned_text.replace('</figref>', '')\n",
    "    \n",
    "    # Lines split correctly\n",
    "    well_split = split_text_with_complex_pattern(cleaned_text)\n",
    "    \n",
    "    # Keywords to filter\n",
    "    keywords = [\"figure\", \"figures\", \"Figure\", \"Figures\", \"FIG.\", \"FIGS.\", \"Fig.\", \"Figs.\"]\n",
    "    \n",
    "    figure_sentences = [line for line in well_split if any(keyword in line for keyword in keywords)]\n",
    "    \n",
    "    lista = []\n",
    "    for text in figure_sentences:\n",
    "        # Case 1: Removing <figref> including brackets and extracting the figure number or range directly.\n",
    "        text = re.sub(r'\\[\\s*<figref idref=\"[^\"]+\">(FIGS?. [0-9a-zA-Z\\(\\)-]+)</figref>\\s*\\]', r'\\1', text)\n",
    "        \n",
    "        # Case 2: Direct replacement of <figref> tags with their content.\n",
    "        text = re.sub(r'<figref idref=\"[^\"]+\">(FIGS?. [0-9a-zA-Z\\(\\)-]+)</figref>', r'\\1', text)\n",
    "\n",
    "        # Case 3: Similar to case 2 but specifically targeting the pattern with brackets at the start.\n",
    "        text = re.sub(r'\\[\\s*<figref idref=\"[^\"]+\">(Fig. [0-9]+)\\s*\\]?\\s*\\1</figref>', r'\\1', text)\n",
    "\n",
    "        # Clean remaining HTML tags but keep their content\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        lista.append(text)\n",
    "\n",
    "    return lista\n",
    "\n",
    "# Pavlo\n",
    "def extraction_paragraphs_with_figures(text, figure_list):\n",
    "    paragraph_pattern = re.compile(r'<p id=\"p(\\d+)\"[^>]*>(.*?)</p>', re.DOTALL)\n",
    "    figref_pattern = re.compile(r'<figref idref=\"f(\\d+)[A-Za-z]*\">(.*?)</figref>', re.DOTALL)\n",
    "    paragraphs_with_figures = {}\n",
    "\n",
    "    if figure_list == [0]:\n",
    "        # If figure_list is [0], include paragraphs with any figure reference\n",
    "        for pnum, ptext in paragraph_pattern.findall(text):\n",
    "            if figref_pattern.search(ptext):  # Check if paragraph contains any figure reference\n",
    "                clean_text = re.sub(r'<[^>]+>', '', ptext).strip()\n",
    "                formatted_text = format_figure_references(clean_text)\n",
    "                paragraphs_with_figures[\"p\" + pnum] = formatted_text\n",
    "    else:\n",
    "        # Otherwise, include paragraphs with specific figures from figure_list\n",
    "        for pnum, ptext in paragraph_pattern.findall(text):\n",
    "            fig_refs = figref_pattern.findall(ptext)\n",
    "            for fig_id, fig_text in fig_refs:\n",
    "                if any(fig in fig_text for fig in figure_list):\n",
    "                    clean_text = re.sub(r'<[^>]+>', '', ptext).strip()\n",
    "                    formatted_text = format_figure_references(clean_text)\n",
    "                    paragraphs_with_figures[\"p\" + pnum] = formatted_text\n",
    "                    break  # Include paragraph once a matching figure is found\n",
    "\n",
    "    return paragraphs_with_figures\n",
    "\n",
    "# Pablo--\n",
    "def split_text_with_complex_pattern(text):\n",
    "    \"\"\"\n",
    "    Splits the given text into lines at periods, handling complex figure references and enumerations without splitting them,\n",
    "    and ensuring correct sentence boundaries are recognized.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "    r'\\.(?!\\s*(Fig|Figs|FIG|FIGS)\\.\\s*\\d+([-,]\\s*\\d+)*\\s+|<\\/?\\s*(li|ul|p|!--)>)\\s*(?=[A-Z])'\n",
    "    )\n",
    "    \n",
    "    # Split the text using the defined pattern\n",
    "    lines = pattern.split(text)\n",
    "    #print(\"Lines:\", lines)\n",
    "    # Ensure that we filter out any None before processing\n",
    "    lines = [line for line in lines if line is not None]\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº A non-empty: 6189\n",
      "Nº Y non-empty: 2232\n",
      "Nº X non-empty: 3671\n",
      "Nº Paragraphs empty 2631\n",
      "Nº A empty: 1058\n",
      "Nº Y empty: 767\n",
      "Nº X empty: 2357\n",
      "Total A + X + Y ~: 12092\n",
      "Total references empty: 4182\n",
      "Total references non-empty : 12092\n"
     ]
    }
   ],
   "source": [
    "# Pavlo and Pablo\n",
    "def process_data(json_mapper, json_citing, json_cited):\n",
    "    # List to store processed data\n",
    "    data = []\n",
    "    \n",
    "    # We count total citing patents\n",
    "    counter_total = 0\n",
    "    \n",
    "    # We count the cases 2 or 3\n",
    "    counter_whole_variations = 0\n",
    "    counter_A_non_empty = 0\n",
    "    counter_Y_non_empty = 0\n",
    "    counter_X_non_empty = 0\n",
    "    counter_paragraphs_empty = 0\n",
    "    counter_A_empty = 0\n",
    "    counter_Y_empty = 0\n",
    "    counter_X_empty = 0\n",
    "    counter_empty = 0\n",
    "    \n",
    "    # Processing each dictionary in \"MAPPING_DATASET_citing_to_citations.json\"\n",
    "    for key, value in json_mapper.items():\n",
    "        for dict_cited in value:\n",
    "            if dict_cited[\"type\"] in [\"X\", \"Y\", \"A\"]:\n",
    "                if not dict_cited[\"claims\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    quadruple_dict = {\n",
    "                        \"ID_Citing\": key,\n",
    "                        \"Category_Cited\": dict_cited[\"type\"],\n",
    "                        \"Claims_Text\": extract_claim_info(json_citing, key, dict_cited[\"claims\"]),\n",
    "                        \"ID_Cited\": dict_cited[\"cited_id\"],\n",
    "                        \"References_Cited\" : dict_cited[\"paragraphs\"],\n",
    "                        \"Content_Cited\": cleaner_cited_references(dict_cited[\"cited_id\"], dict_cited[\"paragraphs\"], json_cited)\n",
    "                    }\n",
    "                    data.append(quadruple_dict)\n",
    "                    \n",
    "                    if quadruple_dict[\"Category_Cited\"] == \"A\" and quadruple_dict[\"Content_Cited\"]:\n",
    "                        counter_A_non_empty +=1\n",
    "                    elif quadruple_dict[\"Category_Cited\"] == \"A\" and not quadruple_dict[\"Content_Cited\"]:\n",
    "                        counter_A_empty +=1\n",
    "                    \n",
    "                    if quadruple_dict[\"Category_Cited\"] == \"X\" and quadruple_dict[\"Content_Cited\"]:\n",
    "                        counter_X_non_empty +=1\n",
    "                    elif quadruple_dict[\"Category_Cited\"] == \"X\" and not quadruple_dict[\"Content_Cited\"]:\n",
    "                        counter_X_empty +=1\n",
    "                    \n",
    "                    if quadruple_dict[\"Category_Cited\"] == \"Y\" and quadruple_dict[\"Content_Cited\"]:\n",
    "                        counter_Y_non_empty +=1\n",
    "                    elif quadruple_dict[\"Category_Cited\"] == \"Y\" and not quadruple_dict[\"Content_Cited\"]:\n",
    "                        counter_Y_empty +=1\n",
    "                    \n",
    "                    if not quadruple_dict[\"References_Cited\"]:\n",
    "                        counter_paragraphs_empty += 1\n",
    "                    if not quadruple_dict[\"Content_Cited\"]:\n",
    "                        counter_empty +=1\n",
    "                    else:\n",
    "                        counter_total +=1   \n",
    "                           \n",
    "    print(\"Nº A non-empty:\", counter_A_non_empty)\n",
    "    print(\"Nº Y non-empty:\", counter_Y_non_empty)\n",
    "    print(\"Nº X non-empty:\", counter_X_non_empty)\n",
    "    print(\"Nº Paragraphs empty\", counter_paragraphs_empty)\n",
    "    print(\"Nº A empty:\", counter_A_empty)\n",
    "    print(\"Nº Y empty:\", counter_Y_empty)\n",
    "    print(\"Nº X empty:\", counter_X_empty)\n",
    "    \n",
    "    print(\"Total A + X + Y ~:\", counter_A_non_empty + counter_X_non_empty + counter_Y_non_empty)\n",
    "    print(\"Total references empty:\", counter_empty)\n",
    "    print(\"Total references non-empty :\", counter_total)\n",
    "    return data\n",
    "\n",
    "# Pavlo and Pablo\n",
    "def open_json_file(path_to_json):\n",
    "    # Opening the JSON file\n",
    "    with open(path_to_json, 'r') as file:\n",
    "        # Loading JSON\n",
    "        json_dict = json.load(file)\n",
    "    return json_dict\n",
    "\n",
    "# Pavlo and Pablo\n",
    "def create_json(data, path):\n",
    "    # Creation of Directory if doesn't exist\n",
    "    directory = '/bigstorage/Pablo_TER'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # File Path to store data information\n",
    "    file_path = os.path.join(directory, 'Q7.json')\n",
    "\n",
    "    # Writing Q7 Information on JSON\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Converting from list of dictionaries to JSON + indent=4 for readability\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Pavlo and Pablo\n",
    "def main():\n",
    "    path_to_json_mapper = \"/bigstorage/ita/clean/private/MAPPING_DATASET_citing_to_citation.json\"\n",
    "    path_to_json = \"/bigstorage/ita/clean/private/CONTENT_DATASET_citing_patents_2020.json\"\n",
    "    path_to_json_content_cited = \"/bigstorage/ita/clean/private/CONTENT_DATASET_cited_patents_by_2020.json\"\n",
    "    #test_paragraphs_path = \"/bigstorage/Pablo_TER/EP_2020_attempts/EP_2020.json\"\n",
    "    path_to_Q7 ='/bigstorage/Pablo_TER'\n",
    "    json_citing = open_json_file(path_to_json)\n",
    "    json_cited = open_json_file(path_to_json_content_cited)\n",
    "    json_mapper = open_json_file(path_to_json_mapper)\n",
    "    data = process_data(json_mapper, json_citing, json_cited)\n",
    "    create_json(data, path_to_Q7)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information related to Q7.JSON\n",
    "\n",
    "- MAPPING_DATASET_citing_to_citations.json -> 20K\n",
    "\n",
    "- Total Cited Patents in 2020: 16274 ( / 20k <- MAPPING_DATASET_citing_to_citations.json) \n",
    "\n",
    "- Total Cited Patents in 2020 with non-empty paragraphs: 12092 (~ 74,30% )\n",
    "\n",
    "- Total Cited Patents in 2020 with empty paragraphs: 4182 (~ 25,7% )\n",
    "\n",
    "- The whole Document:\n",
    "\n",
    "    - \"The Whole Document\" is present in 1752 Cited References\n",
    "\n",
    "    - \"The Whole Document\" + something\" in 87 Cited References (-> in our algorithm we consider 68 / 87 from them)\n",
    "\n",
    "    - \"The Whole Document\" only is in 1588 Cited References\n",
    "\n",
    "- Total Cited Patents in 2020 with empty paragraphs ( paragraphs = \"\" (3079) + paragraphs = \"the whole document\" (1588) + paragraphs = \"figure 1a-4b\" or paragraphs = \"3a-VII\" (1998) ): 6665 (~ 35,62 % )\n",
    "\n",
    "- Total Time Execution for Cited patents in 2020: 2m 21.1s\n",
    "\n",
    "# General Information\n",
    "\n",
    "- MAPPING_DATASET_citing_to_citations.json -> 20K\n",
    "\n",
    "- CONTENT_DATASET_CITED_PATENTS_BY_2020.JSON -> 12.219 ~ 12K\n",
    "\n",
    "- CONTENT_DATASET_CITING_PATENTS_2020.JSON -> 12.093 ~12K\n",
    "\n",
    "- CONTENT_DATASET_DISTINCT_UNCITED_PATENTS.JSON -> 10K\n",
    "\n",
    "- CONTENT_DATASET_UNCITED_PATENTS.JSON -> 10K"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
